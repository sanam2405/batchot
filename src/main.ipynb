{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CV2IbjGBNQ5",
        "outputId": "4080e4b3-eb14-4c02-f527-b9dfa7b73f35"
      },
      "outputs": [],
      "source": [
        "!pip install openai pinecone-client transformers\n",
        "\n",
        "import openai\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDzwTwp3Kka3"
      },
      "outputs": [],
      "source": [
        "OPENAI_API_KEY = 'OPEN_API_KEY'\n",
        "PINECONE_API_KEY = 'PINECONE_API_KEY'\n",
        "PINECONE_CLOUD = 'aws'\n",
        "PINECONE_REGION = 'us-east-1'\n",
        "\n",
        "# Initialize OpenAI client\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "# Initialize Pinecone\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_REGION)\n",
        "\n",
        "MAX_TOKENS = 225\n",
        "PINECONE_INDEX_NAME = 'batchot'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhv0MCH5K7nr"
      },
      "outputs": [],
      "source": [
        "history = [\n",
        "    \"1: User: Hi there! How are you doing today? | Bot: Hello! I'm doing great, thank you! How can I assist you today?\",\n",
        "    \"2: User: What's the weather like today in New York? | Bot: Today in New York, it's sunny with a slight chance of rain.\",\n",
        "    \"3: User: Great! Do you have any good lunch suggestions? | Bot: Sure! How about trying a new salad recipe?\",\n",
        "    \"4: User: That sounds healthy. Any specific recipes? | Bot: You could try a quinoa salad with avocado and chicken.\",\n",
        "    \"5: User: Sounds delicious! I'll try it. What about dinner? | Bot: For dinner, you could make grilled salmon with vegetables.\",\n",
        "    \"6: User: Thanks for the suggestions! Any dessert ideas? | Bot: How about a simple fruit salad or yogurt with honey?\",\n",
        "    \"7: User: Perfect! Now, what are some good exercises? | Bot: You can try a mix of cardio and strength training exercises.\",\n",
        "    \"8: User: Any specific recommendations for cardio? | Bot: Running, cycling, and swimming are all excellent cardio exercises.\",\n",
        "    \"9: User: I'll start with running. Can you recommend any books? | Bot: 'Atomic Habits' by James Clear is a highly recommended book.\",\n",
        "    \"10: User: I'll check it out. What hobbies can I take up? | Bot: You could explore painting, hiking, or learning a new instrument.\",\n",
        "    \"11: User: Hiking sounds fun! Any specific trails? | Bot: There are great trails in the Rockies and the Appalachian Mountains.\",\n",
        "    \"12: User: I'll plan a trip. What about indoor activities? | Bot: Indoor activities like reading, cooking, or playing board games.\",\n",
        "    \"13: User: Nice! Any good board games? | Bot: Settlers of Catan and Ticket to Ride are both excellent choices.\",\n",
        "    \"14: User: I'll try them out. Any movie recommendations? | Bot: 'Inception' and 'The Matrix' are must-watch movies.\",\n",
        "    \"15: User: I love those movies! Any TV shows? | Bot: 'Breaking Bad' and 'Stranger Things' are very popular.\",\n",
        "    \"16: User: Great choices! What about podcasts? | Bot: 'How I Built This' and 'The Daily' are very informative.\",\n",
        "    \"17: User: Thanks! What are some good travel destinations? | Bot: Paris, Tokyo, and Bali are amazing travel spots.\",\n",
        "    \"18: User: I'll add them to my list. Any packing tips? | Bot: Roll your clothes to save space and use packing cubes.\",\n",
        "    \"19: User: That's helpful! What about travel insurance? | Bot: Always get travel insurance for safety and peace of mind.\",\n",
        "    \"20: User: Thanks for the tips! Any last advice? | Bot: Just enjoy your journey and make the most out of your experiences.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABJICJvkLBks",
        "outputId": "4b341a44-7f5f-4479-929a-d297096e6373"
      },
      "outputs": [],
      "source": [
        "def add_embeddings_to_pinecone(history, index_name=PINECONE_INDEX_NAME):\n",
        "    # Initialize the tokenizer and model\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    model = GPT2Model.from_pretrained('gpt2')\n",
        "\n",
        "    # Create Pinecone index if it doesn't exist\n",
        "    if index_name not in pc.list_indexes():\n",
        "        print(f\"Existing indexes: {pc.list_indexes()}\")\n",
        "        try:\n",
        "          pc.create_index(\n",
        "              name=index_name,\n",
        "              dimension=768,\n",
        "              metric=\"cosine\",\n",
        "              spec=ServerlessSpec(\n",
        "                cloud=\"aws\",\n",
        "                region=\"us-east-1\"\n",
        "              )\n",
        "            )\n",
        "          print(f\"Index {index_name} created successfully.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating index: {e}\")\n",
        "            return\n",
        "\n",
        "    index = pc.Index(index_name)\n",
        "\n",
        "    # Encode and upsert each message\n",
        "    for i, message in enumerate(history):\n",
        "        inputs = tokenizer(message, return_tensors='pt', truncation=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()\n",
        "\n",
        "        try:\n",
        "            index.upsert(vectors=[(str(i), embedding, {\"text\": message})])\n",
        "            print(f\"Inserted message {i}: {message}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error upserting vector {i}: {e}\")\n",
        "\n",
        "add_embeddings_to_pinecone(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAZK37I4LFkK"
      },
      "outputs": [],
      "source": [
        "def retrieve_relevant_history(query, index_name=PINECONE_INDEX_NAME):\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    model = GPT2Model.from_pretrained('gpt2')\n",
        "\n",
        "    inputs = tokenizer(query, return_tensors='pt', truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    query_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()\n",
        "\n",
        "    index = pc.Index(index_name)\n",
        "    results = index.query(vector=query_embedding, top_k=3, include_metadata=True)\n",
        "\n",
        "    return [result.metadata['text'] for result in results.matches]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fimRdRCJLJHd"
      },
      "outputs": [],
      "source": [
        "def prepare_prompt(test_prompt, history, index_name=PINECONE_INDEX_NAME):\n",
        "    relevant_messages = retrieve_relevant_history(test_prompt, index_name)\n",
        "    context = \"\\n\".join(relevant_messages)\n",
        "    combined_prompt = f\"{context}\\nUser: {test_prompt}\\nBot:\"\n",
        "\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    tokens = tokenizer.encode(combined_prompt)\n",
        "\n",
        "    if len(tokens) > MAX_TOKENS:\n",
        "        tokens = tokens[-MAX_TOKENS:]\n",
        "        combined_prompt = tokenizer.decode(tokens)\n",
        "\n",
        "    return combined_prompt, context, tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "ZPcAQ5WwLNCi",
        "outputId": "1337ea8d-e436-43f9-881b-511501b21333"
      },
      "outputs": [],
      "source": [
        "def test_final_prompt():\n",
        "    final_test_prompt = \"Do you think it will help me stay fit?\"\n",
        "\n",
        "    prepared_prompt, context_referred, tokenizer = prepare_prompt(final_test_prompt, history)\n",
        "\n",
        "    response = openai.chat.completions.create(\n",
        "        # model=\"gpt-3.5-turbo\",\n",
        "        # model=\"gpt-3.5-turbo-1106\",\n",
        "        # model=\"gpt-4o-mini\",\n",
        "        # model=\"text-embedding-3-small\",\n",
        "        model=\"gpt-4\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prepared_prompt}\n",
        "        ],\n",
        "        max_tokens=MAX_TOKENS - len(tokenizer.encode(prepared_prompt))\n",
        "    )\n",
        "\n",
        "    print(f\"Final Test Prompt: {final_test_prompt}\")\n",
        "    print(f\"Context Referred: {context_referred}\")\n",
        "    print(f\"Final Test Prompt Response: {response.choices[0].message['content'].strip()}\")\n",
        "\n",
        "# Call the test function to generate the Final Test Prompt Response\n",
        "test_final_prompt()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
