{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXZua6NhDTqZ",
        "outputId": "0bd3e1b8-44b6-46ff-e2c1-8dbe5382533c"
      },
      "outputs": [],
      "source": [
        "# !pip install openai pinecone-client transformers\n",
        "\n",
        "# Import necessary libraries\n",
        "import openai  # For interacting with OpenAI's API\n",
        "from pinecone import Pinecone, ServerlessSpec  # For vector database operations using Pinecone\n",
        "from transformers import GPT2Tokenizer, GPT2Model  # For text tokenization and embedding generation using GPT-2\n",
        "import torch  # For numerical operations and tensor manipulation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8vHkhyybFp_"
      },
      "outputs": [],
      "source": [
        "# Set your OpenAI and Pinecone API keys\n",
        "OPENAI_API_KEY = 'OPENAI_API_KEY'  # Replace with your actual OpenAI API key\n",
        "PINECONE_API_KEY = 'PINECONE_API_KEY'  # Replace with your actual Pinecone API key\n",
        "PINECONE_CLOUD = 'aws'  # Specify the cloud provider for your Pinecone instance\n",
        "PINECONE_REGION = 'us-east-1'  # Specify the region for your Pinecone instance\n",
        "\n",
        "# Initialize OpenAI client\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "# Initialize Pinecone client\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_REGION)\n",
        "\n",
        "# Set the maximum number of tokens for the prompt\n",
        "MAX_TOKENS = 225\n",
        "# Define the name for the Pinecone index\n",
        "PINECONE_INDEX_NAME = 'batchot'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70Clv_lpbQiX"
      },
      "outputs": [],
      "source": [
        "# Sample conversation history for demonstration\n",
        "history = [\n",
        "    \"1: User: Hi there! How are you doing today? | Bot: Hello! I'm doing great, thank you! How can I assist you today?\",\n",
        "    \"2: User: What's the weather like today in New York? | Bot: Today in New York, it's sunny with a slight chance of rain.\",\n",
        "    \"3: User: Great! Do you have any good lunch suggestions? | Bot: Sure! How about trying a new salad recipe?\",\n",
        "    \"4: User: That sounds healthy. Any specific recipes? | Bot: You could try a quinoa salad with avocado and chicken.\",\n",
        "    \"5: User: Sounds delicious! I'll try it. What about dinner? | Bot: For dinner, you could make grilled salmon with vegetables.\",\n",
        "    \"6: User: Thanks for the suggestions! Any dessert ideas? | Bot: How about a simple fruit salad or yogurt with honey?\",\n",
        "    \"7: User: Perfect! Now, what are some good exercises? | Bot: You can try a mix of cardio and strength training exercises.\",\n",
        "    \"8: User: Any specific recommendations for cardio? | Bot: Running, cycling, and swimming are all excellent cardio exercises.\",\n",
        "    \"9: User: I'll start with running. Can you recommend any books? | Bot: 'Atomic Habits' by James Clear is a highly recommended book.\",\n",
        "    \"10: User: I'll check it out. What hobbies can I take up? | Bot: You could explore painting, hiking, or learning a new instrument.\",\n",
        "    \"11: User: Hiking sounds fun! Any specific trails? | Bot: There are great trails in the Rockies and the Appalachian Mountains.\",\n",
        "    \"12: User: I'll plan a trip. What about indoor activities? | Bot: Indoor activities like reading, cooking, or playing board games.\",\n",
        "    \"13: User: Nice! Any good board games? | Bot: Settlers of Catan and Ticket to Ride are both excellent choices.\",\n",
        "    \"14: User: I'll try them out. Any movie recommendations? | Bot: 'Inception' and 'The Matrix' are must-watch movies.\",\n",
        "    \"15: User: I love those movies! Any TV shows? | Bot: 'Breaking Bad' and 'Stranger Things' are very popular.\",\n",
        "    \"16: User: Great choices! What about podcasts? | Bot: 'How I Built This' and 'The Daily' are very informative.\",\n",
        "    \"17: User: Thanks! What are some good travel destinations? | Bot: Paris, Tokyo, and Bali are amazing travel spots.\",\n",
        "    \"18: User: I'll add them to my list. Any packing tips? | Bot: Roll your clothes to save space and use packing cubes.\",\n",
        "    \"19: User: That's helpful! What about travel insurance? | Bot: Always get travel insurance for safety and peace of mind.\",\n",
        "    \"20: User: Thanks for the tips! Any last advice? | Bot: Just enjoy your journey and make the most out of your experiences.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeOKAeb0bU9I",
        "outputId": "0d797666-13e8-4602-acaf-1728454fc509"
      },
      "outputs": [],
      "source": [
        "# Define a function to add embeddings to Pinecone index\n",
        "def add_embeddings_to_pinecone(history, index_name=PINECONE_INDEX_NAME):\n",
        "    \"\"\"\n",
        "    Adds embeddings for each message in the history to the specified Pinecone index.\n",
        "\n",
        "    Args:\n",
        "        history (list): A list of conversation messages.\n",
        "        index_name (str, optional): The name of the Pinecone index. Defaults to PINECONE_INDEX_NAME.\n",
        "\n",
        "    The function does the following:\n",
        "    1. Initializes the GPT-2 tokenizer and model\n",
        "    2. Creates a Pinecone index if it doesn't exist\n",
        "    3. Encodes each message in the history and creates an embedding\n",
        "    4. Upserts the embedding and associated metadata to Pinecone\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize the GPT-2 tokenizer and model for text processing and embedding generation\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    model = GPT2Model.from_pretrained('gpt2')\n",
        "\n",
        "    # Check if the specified index exists in Pinecone\n",
        "    if index_name not in pc.list_indexes():\n",
        "        print(f\"Existing indexes: {pc.list_indexes()}\")\n",
        "        try:\n",
        "            # Create a new index in Pinecone if it doesn't exist\n",
        "            pc.create_index(\n",
        "                name=index_name,  # Name of the index\n",
        "                dimension=768,  # Dimensionality of the embeddings (768 for GPT-2)\n",
        "                metric=\"cosine\",  # Distance metric for similarity search (cosine similarity)\n",
        "                spec=ServerlessSpec(\n",
        "                    cloud=\"aws\",  # Cloud provider for the index\n",
        "                    region=\"us-east-1\"  # Region for the index\n",
        "                )\n",
        "            )\n",
        "            print(f\"Index {index_name} created successfully.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating index: {e}\")\n",
        "            return\n",
        "\n",
        "    # Connect to the Pinecone index\n",
        "    index = pc.Index(index_name)\n",
        "\n",
        "    # Iterate through each message in the history\n",
        "    for i, message in enumerate(history):\n",
        "        # Tokenize the message using the GPT-2 tokenizer\n",
        "        inputs = tokenizer(message, return_tensors='pt', truncation=True, max_length=512)\n",
        "        # Generate embeddings for the tokenized message using the GPT-2 model\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        # Extract the embedding vector as a list\n",
        "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()\n",
        "\n",
        "        try:\n",
        "            # Upsert the embedding vector along with the message text into the Pinecone index\n",
        "            index.upsert(vectors=[(str(i), embedding, {\"text\": message})])\n",
        "            print(f\"Inserted message {i}: {message}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error upserting vector {i}: {e}\")\n",
        "\n",
        "# Call the function to add embeddings for the sample conversation history\n",
        "add_embeddings_to_pinecone(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w84nYZQebVoH"
      },
      "outputs": [],
      "source": [
        "# Define a function to retrieve relevant history from Pinecone index\n",
        "def retrieve_relevant_history(query, index_name=PINECONE_INDEX_NAME):\n",
        "    \"\"\"\n",
        "    Retrieves relevant history messages from the Pinecone index based on the query.\n",
        "\n",
        "    Args:\n",
        "        query (str): The user's query.\n",
        "        index_name (str, optional): The name of the Pinecone index. Defaults to PINECONE_INDEX_NAME.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of relevant history messages.\n",
        "\n",
        "    The function does the following:\n",
        "    1. Encodes the query using GPT-2\n",
        "    2. Searches for similar vectors in Pinecone\n",
        "    3. Returns the text of the most similar entries\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize the GPT-2 tokenizer and model for text processing and embedding generation\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    model = GPT2Model.from_pretrained('gpt2')\n",
        "\n",
        "    # Tokenize the query using the GPT-2 tokenizer\n",
        "    inputs = tokenizer(query, return_tensors='pt', truncation=True, max_length=512)\n",
        "    # Generate embeddings for the tokenized query using the GPT-2 model\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    # Extract the query embedding vector as a list\n",
        "    query_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()\n",
        "\n",
        "    # Connect to the Pinecone index\n",
        "    index = pc.Index(index_name)\n",
        "    # Query the Pinecone index for the top 3 most similar messages to the query embedding\n",
        "    results = index.query(vector=query_embedding, top_k=3, include_metadata=True)\n",
        "\n",
        "    # Extract and return the text content of the retrieved messages\n",
        "    return [result.metadata['text'] for result in results.matches]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0k5f3aBbbbM"
      },
      "outputs": [],
      "source": [
        "# Define a function to prepare the prompt for the OpenAI model\n",
        "def prepare_prompt(test_prompt, history, index_name=PINECONE_INDEX_NAME):\n",
        "    \"\"\"\n",
        "    Prepares the prompt for the OpenAI model by retrieving relevant history and combining it with the test prompt.\n",
        "\n",
        "    Args:\n",
        "        test_prompt (str): The user's test prompt.\n",
        "        history (list): A list of conversation messages.\n",
        "        index_name (str, optional): The name of the Pinecone index. Defaults to PINECONE_INDEX_NAME.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the combined prompt, the context referred to, and the tokenizer.\n",
        "\n",
        "    The function does the following:\n",
        "    1. Retrieves relevant history based on the query\n",
        "    2. Combines the relevant history with the user's query\n",
        "    3. Truncates the prompt if it exceeds the maximum token limit\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve relevant history messages based on the test prompt\n",
        "    relevant_messages = retrieve_relevant_history(test_prompt, index_name)\n",
        "    # Combine the relevant messages into a single string\n",
        "    context = \"\\n\".join(relevant_messages)\n",
        "    # Construct the combined prompt by appending the context, test prompt, and \"Bot:\"\n",
        "    combined_prompt = f\"{context}\\nUser: {test_prompt}\\nBot:\"\n",
        "\n",
        "    # Initialize the GPT-2 tokenizer\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    # Encode the combined prompt using the tokenizer\n",
        "    tokens = tokenizer.encode(combined_prompt)\n",
        "\n",
        "    # Truncate the combined prompt if it exceeds the maximum token limit\n",
        "    if len(tokens) > MAX_TOKENS:\n",
        "        tokens = tokens[-MAX_TOKENS:]\n",
        "        combined_prompt = tokenizer.decode(tokens)\n",
        "\n",
        "    # Return the combined prompt, context referred to, and the tokenizer\n",
        "    return combined_prompt, context, tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "mzErAA6VbeMC",
        "outputId": "1bef25d6-1f43-48d4-c8fd-b6f195791d3c"
      },
      "outputs": [],
      "source": [
        "# Define a function to test the final prompt\n",
        "def test_final_prompt():\n",
        "    \"\"\"\n",
        "    This function tests the entire pipeline by generating a response to a test prompt.\n",
        "\n",
        "    The function does the following:\n",
        "    1. Defines a test prompt\n",
        "    2. Prepares the prompt using the prepare_prompt function\n",
        "    3. Sends the prepared prompt to the OpenAI API\n",
        "    4. Prints the results, including the context referred and the model's response\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the final test prompt\n",
        "    final_test_prompt = \"Do you think it will help me stay fit?\"\n",
        "\n",
        "    # Prepare the prompt for the OpenAI model\n",
        "    prepared_prompt, context_referred, tokenizer = prepare_prompt(final_test_prompt, history)\n",
        "    print(prepared_prompt)\n",
        "    print(\"##########################\")\n",
        "\n",
        "    # Generate a response from the OpenAI model\n",
        "    response = openai.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",  # Specify the OpenAI model to use\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},  # System message to set the assistant's behavior\n",
        "            {\"role\": \"user\", \"content\": prepared_prompt}  # User message containing the prepared prompt\n",
        "        ],\n",
        "        max_tokens=MAX_TOKENS - len(tokenizer.encode(prepared_prompt))  # Limit the response length\n",
        "    )\n",
        "\n",
        "    # Print the final test prompt, context referred to, and the model's response\n",
        "    print(f\"Final Test Prompt: {final_test_prompt}\")\n",
        "    print(f\"Context Referred: {context_referred}\")\n",
        "    print(f\"Final Test Prompt Response: {response.choices[0].message['content'].strip()}\")\n",
        "\n",
        "# Call the test function to generate the Final Test Prompt Response\n",
        "test_final_prompt()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
